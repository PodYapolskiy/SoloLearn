Что делает правильное разбиение:
    Нам нужно оценивать каждое разбиение и выбрать то, которое наберёт больше всего баллов.

    Мерилом этого будет выступать информационный прирост (information gain). Это будет значением от 0 до 1, когда ноль - инф. прирост бесполезного разбиения и единица - идеального.

    Некоторые другие метрики - gini impurity и энтропия (entropy).


    При разбиении мы стремимся к однородности (homogeneity) с каждой стороны. В идеале отправить всех выживших пассажиров в одну сторону, а погибших в другую.
    Набор считается чистым (purе), когда все точки соответсвуют одному признаку.


Gini umpurity (нечистота Джини):
    Мера того, насколько чистая выборка. В данном случае сколько точек данных выживших и невыживших.
    Это будет значением от 0 до 0.5, где 0.5 - очень неотчётливо, а 0 - идельная выборка.
    p - Процент выживших пассажиров. 1 - p соответственно невыживших.

    gini = 2 * p * (1 - p)

    Посчитаем на примере (0.3.png):
        Для Age <= 30: 
            Процент выживших: 197 / (197 + 328) = 0.3752
            Процент погибших: 1 - 0.3752 = 0.6248

            gini = 2 * 0.3752 * 0.6248 = 0.4689

        Для Age > 30:

            gini = 2 * 145/(145+217) * 217/(145+217) = 0.4802
        
        Оба значения близки к 0.5 - плохой пример разбиения.


    Теперь пример с разбиение по полу (0.4.png):
        Для женщин:
            gini = 2 * 233/(233+81) * 81/(233+81) = 0.3828
        
        Для мужчин:
            gini = 2 * 109/(109+464) * 464/(109+464) = 0.3081
        
        Эти значения получились более удовлетворительными, чем при разбиению по возрасту.


Entropy (Энтропия):
    Мера отчётливости. Значение от 0 до 1, где 1 - полная нечистота, а 0 - идеально чистое разбиение.

    entropy = - (p * log2(p) + (1 - p) * log2(1 - p))

    Пример (0.3.png):
        Age <= 30
        entropy = -(0.375 * log(0.375) + (1-0.375) * log(1-0.375)) = 0.954

        Age > 30
        entropy = -(0.401 * log(0.401) + (1-0.401) * log(1-0.401)) =  0.9713

        Очень нечистые разбиения
    
    Пример (0.4.png):
        Female
        entropy = -(p * log(p) + (1-p) * log(1-p)) = 0.8237

        Male
        Entropy =  -(p * log(p) + (1-p) * log(1-p)) = 0.7019

        Разбиение лучше, чем в примере с возростом.


Information gain (инф. прирост):
                               |A|            |B|
    Information gain = H(S) - ----- * H(A) - ----- * H(B)
                               |S|            |S|
    
    H - мера нечистоты (gini или entropy). S - основной датасет; A и B - 2 датасета, на которые мы разделили S.

    gini всего датасета = 0.4738

    Пример для разбиения по возрасту:
        information gain = 0.4738 - 525/887 * 0.4689 - 362/887 * 0.4802 = 0.0003

        Значение очень маленькое, означающее бесполезность такого разбиения

    Пример с разбиением по полу:
        information gain = 0.4738 - 314/887 * 0.3828 - 573/887 * 0.3081 = 0.1393

        Опять же значение лучше, чем при разбиению по возрасту


Построение дерева решений:
    1. Рассматриваем все возможные разбиения дабы выислить наилучшее.
    2. Для чиловых свойств (Age, Pclass, ...) мы перебираем все возможные пороги.
        Имея 89 различных возростов нашем датасете, можем составить 88 разных разбиений только с возрастом

    Перебираем потенциальные разбиения:
        1. Sex (male | female)
        2. Pclass (1 or 2 | 3)
        3. Pclass (1 | 2 or 3)
        4. Age (0 | >0)
        5. Age (<=1 | >1)
        6. Age (<=2 | >2)
        7. etc….

    Кол-во разбиений:
        Для пола - 1
        Для разделения по классу обслуживания - 2
        Для возраста имеем - 88
        Для цены за проезд  - 247

        Всего = 338, если брать только эти 4 свойства

    Проводя вычисления, выбираем разбиение с наивысшим Information gain.
