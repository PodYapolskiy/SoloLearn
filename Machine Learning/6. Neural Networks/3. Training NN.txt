Потеря, проигрыш (Loss):
    Чтобы тренировать нейронную сеть, нужно найти функцию потерь (loss function).
    Это мера того насколько наша нейронка далека от идеала.
    Когда мы тренируем НС, мы оптимизируем loss function.

    Мы будем использовать перекрёстную энтропию (cross entropy) loss function.
    Схоже с likelihood из Линейной регрессии.

                    p       if y = 1
    cross entropy =  and
                    1 - p   if y = 0

    Мы умножаем все значения cross entropy для всех точек и получаем общую CE.

    Пример (3.png):
        Cross entropy 1-ой модели: 0.6 * 0.8 * (1 - 0.3) * (1 - 0.4) = 0.2016
        Cross entropy 2-ой модели: 0.5 * 0.9 * (1 - 0.1) * (1 - 0.5) = 0.2025
    
    Эта метрика будет становиться больше, если модель будет лучше.


Обратное распространение (Backpropagation):
    Нейронная сеть обучается задом наперёд начиная с выходного узла обновляет коэфиценты. Этот процесс и называется обратным распространением (backpropagation | backprop).

    Идея в том, чтобы инициализировать значения коэффицентов и итеративно изменять значения, чтобы на каждой итерации видеть улучшение loss function. Когда улучшение loss function уже будет невозможно, это будет значить, что мы нашли оптимальную модель.


Алгоритм:
    1) Фиксируем кол-во узлов и слоёв
    2) Используем backprop для итеративного обновления всех значений коэффициентов, пока не сойдемся на оптимальной нейронной сети.
